DataFrames and SQL
DataFrame syntax is more flexible than SQL syntax. Here we illustrate general usage patterns of SQL and DataFrames.

Suppose we have a data set we loaded as a table called myTable and an equivalent DataFrame, called df. We have three fields/columns called col_1 (numeric type), col_2 (string type) and col_3 (timestamp type) Here are basic SQL operations and their DataFrame equivalents.

Notice that columns in DataFrames are referenced by col("").

SQL	DataFrame (Python)
SELECT col_1 FROM myTable	df.select(col("col_1"))
DESCRIBE myTable	df.printSchema()
SELECT * FROM myTable WHERE col_1 > 0	df.filter(col("col_1") > 0)
..GROUP BY col_2	..groupBy(col("col_2"))
..ORDER BY col_2	..orderBy(col("col_2"))
..WHERE year(col_3) > 1990	..filter(year(col("col_3")) > 1990)
SELECT * FROM myTable LIMIT 10	df.limit(10)
display(myTable) (text format)	df.show()
display(myTable) (html format)	display(df)
Hint Hint: You can also run SQL queries with the special syntax spark.sql("SELECT * FROM myTable")


weblogData = (spark
              .read
              .option("header","true") # Allows us to extract the header
              .option("delimiter", '|') # Specifies the pipe character as the delimiter
              .csv("mnt/dw-training/retaildata/rawdata/weblognew/[3-5]/{*}/weblog.txt"))
weblogData.show()


In the above cell, we are trying to read data from the mounted path:

/retaildata/rawdata/weblognew/\[3-5\]/{\*}/weblog.txt

We can use wild card characters in a path string to read multiple files.

[3-5] matches with all the sub directories inside the weblognew directory having names 3, 4 and 5.

* matches with all the sub directories inside parent directory.

The above code returns a DataFrame and sets it to the variable weblogData.

DataFrame.take( n ) returns the first n number of elements from a DataFrame.


display(weblogData.take(5))


from pyspark.sql.functions import col
users = (spark
         .read
         .option("inferSchema", "true") # Automatically applies a schema by evaluating the data types
         .option("delimiter", ',')
         .csv("mnt/dw-training/retaildata/rawdata/UserFile/part{*}")
         .select(col("_c0").alias("UserId"),
                 col("_c3").alias("FirstName"),
                 col("_c5").alias("LastName"),
                 col("_c9").alias("Gender"),
                 col("_c15").alias("Age"),
                 col("_c18").alias("RegisteredDate")
                )
        )
products = (spark
            .read
            .option("inferSchema", "true")
            .option("delimiter", ',')
            .csv("mnt/dw-training/retaildata/rawdata/ProductFile/part{*}")
            .select(col("_c0").alias("ProductId"),
                    col("_c1").alias("ProductName"),
                    col("_c2").alias("BasePrice"),
                    col("_c3").alias("CategoryId"),
                    col("_c7").alias("Category"),
                    col("_c8").alias("Department")
                   )
           )
           
           products.orderBy("ProductId",ascending=False).show(5)
           products.orderBy(["ProductName","BasePrice"],ascending=[0, 1]).show(5)
           products.orderBy(products.ProductId.desc()).show(5)
           products.count()
           products.agg({"BasePrice" : "max"}).show(1)
           
           
           from pyspark.sql import functions as F

           products.agg(F.min(products.BasePrice)).show(1)
           
           products.where(products.BasePrice > 500).show(5)
           # Without column name
           products.groupBy().min().show()
           # With column name
           products.groupBy("Department").count().show(5)
           # Get minimum BasePrice in each department
            products.groupBy(products.Department).min("BasePrice").show(5)
            weblogSampleData = weblogData.where(weblogData.Action == "Purchased").take(10)
            weblogSampleRDD = sc.parallelize(weblogSampleData)
            weblogSampleDF = weblogSampleRDD.toDF()
            
            Join
join(other, joinExprs=None, joinType=None)

Joins with another DataFrame, using the given join expression.

Parameters:

other -- Right side of the join

joinExprs -- a string for join column name, or a join expression (Column). If joinExprs is a string indicating the name of the join column, the column must exist on both sides, and this performs an inner equi-join.

joinType -- str, default 'inner'. One of inner, outer, left_outer, right_outer, semijoin.

Execute the following cell to join the sample weblog DataFrame with the products DataFrame based on the Product ID key to get the BasePrice for each product purchased in the weblog DataFrame:

weblogSampleDF.join(products,weblogSampleDF.ProductId == products.ProductId,'inner').select(weblogSampleDF.SessionId,weblogSampleDF.ProductId,weblogSampleDF.Quantity,products.BasePrice).show(10)

weblogSampleDF.join(products,weblogSampleDF.ProductId == products.ProductId,'inner').select(weblogSampleDF.SessionId,weblogSampleDF.ProductId,weblogSampleDF.Quantity,products.BasePrice).show(10)


Run SQL queries
Execute the following cells to learn how to run Spark SQL queries.

Notice that each cell starts with the %sql magic. This tells the Spark engine to execute the cell using the SQL language, as opposed to the default python language.


%sql
select * from products
limit 10

%sql
select ProductName from products where ProductId = 30
1
%sql
select Department, count(ProductId) as ProductCount
from products group by Department order by ProductCount desc

%sql
select w.SessionId, w.ProductId, p.ProductName, p.BasePrice, w.Quantity, (p.BasePrice * w.Quantity) as Total
from weblogs w Join products p on w.ProductId == p.ProductId

%sql

select w.SessionId, w.ProductId, p.ProductName, p.BasePrice, w.Quantity, (p.BasePrice * w.Quantity) as Total
from weblogs w Join products p on w.ProductId == p.ProductId

%sql

SELECT COUNT(UserId), Year(RegisteredDate) as RegistrationDate FROM users
GROUP BY Year(RegisteredDate) ORDER BY Year(RegisteredDate)


           
           
