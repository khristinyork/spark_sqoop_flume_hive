#CLOUDERA EJERCICIOS.

/ Contribution from Raphael L. Nascimento: 
sqlContext.sql("SET spark.sql.parquet.compression.codec=snappy")

#ARRANCAR DOCKER DE CLOUDERA
docker run --hostname=quickstart.cloudera --privileged=true -t -i -v /home/docker-clodera:/src --publish-all=true -p 8088:8088  -p 8888:8888 -p 50090:50090 -p 50070:50070 -p 50075:50075 -p 8042:8042 -p 60030:60030 -p 25000:25000 -p 25010:25010 -p 18088:18088 -p 8983:8983 -p 11000:11000 cloudera/quickstart /usr/bin/docker-quickstart



PARA SABER EL TAMAÑO DE UN RDD EN MEMORIA
import org.apache.spark.util.SizeEstimator
println(SizeEstimator.estimate(rdd1));


1.-Planteamiento del problema:Usando sqoop, importe la tabla de órdenes en hdfs a folders 
 / user / cloudera / problem1 / orders . 
 El archivo debe cargarse como archivo Avro y usar compresión rápida.
 
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --target-dir /user/cloudera/problema1/orders --as-avrodatafile;
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problema1/orders_nocopression_snnapy --as-avrodatafile;
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --target-dir /user/cloudera/problema1/ordersTextFile --as-textfile;
sqoop-import  --connect jdbc:mysql://quickstart.cloudera:3306/retail_db  --username root --password cloudera --table order_items --compress  --compression-codec org.apache.hadoop.io.compress.SnappyCodec  --target-dir /user/cloudera/problema1/order-items --as-avrodatafile;
hadoop fs -ls -R /user/cloudera/problema1drwxr-xr-x   - root cloudera          0 2018-03-12 07:40 /user/cloudera/problema1/orders
-rw-r--r--   1 root cloudera          0 2018-03-12 07:40 /user/cloudera/problema1/orders/_SUCCESS
-rw-r--r--   1 root cloudera     164082 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00000.avro
-rw-r--r--   1 root cloudera     164153 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00001.avro
-rw-r--r--   1 root cloudera     164269 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00002.avro
-rw-r--r--   1 root cloudera     169328 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00003.avro
drwxr-xr-x   - root cloudera          0 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile
-rw-r--r--   1 root cloudera          0 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/_SUCCESS
-rw-r--r--   1 root cloudera     218873 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00000.snappy
-rw-r--r--   1 root cloudera     218610 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00001.snappy
-rw-r--r--   1 root cloudera     220683 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00002.snappy
-rw-r--r--   1 root cloudera     233412 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00003.snappy
drwxr-xr-x   - root cloudera          0 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy
-rw-r--r--   1 root cloudera          0 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/_SUCCESS
-rw-r--r--   1 root cloudera     439146 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00000.avro
-rw-r--r--   1 root cloudera     447726 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00001.avro
-rw-r--r--   1 root cloudera     446959 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00002.avro
-rw-r--r--   1 root cloudera     447606 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00003.avro



select table_schema "DATABASE",TABLE_NAME "tableName",convert(sum(data_length+index_length)/1048576,decimal(6,2)) "SIZE (MB)" from  information_schema.tables where   table_schema="retail_db" group by  table_schema,TABLE_NAME;

2. Problema 2  Utilizando Sqoop, importar la tabla “order_items” en el directorio de HDFS /user/cloudera/problema1/order_items. El tipo de fichero debe de ser Avro y el tipo de compresión Snappy. 

sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table order_items  --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec  --target-dir /user/cloudera/problema1/order-items  --as-avrodatafile; 
 
 
 
 2.2.- Problema 4  Vamos a generar resultados intermedios con el siguiente formato de columnas:
 ● order_date (fecha del pedido)  ● order_status (estado del pedido)  ● total_orders (número total de pedidos) ● total_amount (suma total ) 
 
El resultado debería ordenarse por: ● order_date  en orden descendente ● order_status  ascendente  ● total_amount  en descendente ● total_orders  en ascendente. 
 
El objetivo es encontrar el número total de orders y la cantidad total (amount) por estado y por día. 
 
Las agregaciones deberían realizarse utilizando los métodos descritos más abajo: a) Utilizando la API de DataFrames - order_date debería tener el formato YYYY-MM-DD b) Utilizando Spark SQL - order_date debería tener el formato YYYY-MM-DD c) Utilizando la función CombineByKey en los RDDs. No es necesario formatear el campo order_date o total_amount. 
 
Lo primero de todos vamos a unir los dos conjuntos de datos para tener un dataframe con todas las columnas necesarias. 


[root@quickstart /]# spark-shell

scala>
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val orders = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problema1/orders")
val order_items = sqlContext.read.avro("/user/cloudera/problema1/order-items") 
val order_items = sqlContext.read.avro("/user/cloudera/problema1/order-items") 

Crear una tabla en mysql 
#mysql -uroot -pcloudera
mysql>create table retail_db.result(ordered_date varchar(255) not null, order_status varchar(255) not null,total_orders int not null,total_amount numeric not null );

//SQOOP importar un fichero de HDFS a una tabla de mysql
sqoop export --table result --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba --password cloudera --export-dir "/user/cloudera/problem1/result4a-csv" --columns "order_date,order_status,total_amount,total_orders"
//SQOOP exportar de una tabla de mysql a hdfs
sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table order_items  --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec  --target-dir /user/cloudera/problema1/order-items  --as-avrodatafile;  

//EJERCICIO 2 
1)utilizando SQOOP copia los datos de la tabla productos a un directorio de HDFS separando los campos por "|"

sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table products  --as-textfile  --target-dir /user/cloudera/products --fields-terminated-by '|'  ;  



Table 5. Output line formatting arguments:

Argument	Description
--enclosed-by <char>	          Sets a required field enclosing character
--escaped-by <char>	              Sets the escape character
--fields-terminated-by <char>	  Sets the field separator character
--lines-terminated-by <char>	  Sets the end-of-line character
--mysql-delimiters	              Uses MySQL’s default delimiter set: fields: , lines: \n escaped-by: \ optionally-enclosed-by: '
--optionally-enclosed-by <char>	  Sets a field enclosing character

Delimiters may be specified as:

a character (--fields-terminated-by X)
an escape character (--fields-terminated-by \t). Supported escape characters are:

\b (backspace)
\n (newline)
\r (carriage return)
\t (tab)
\" (double-quote)
\\' (single-quote)
\\ (backslash)
\0 (NUL) - This will insert NUL characters between fields or lines, or will disable enclosing/escaping if used for one of the --enclosed-by, --optionally-enclosed-by, or --escaped-by arguments.


2)move all the files from /user/cloudera/products folder to /user/cloudera/problem2/products folder
[root@quickstart /]# hadoop fs -mkdir /user/cloudera/problem2
[root@quickstart /]# hadoop fs -mkdir /user/cloudera/problem2/products
[root@quickstart /]# hadoop fs -ls /user/cloudera/problem2/products

[root@quickstart /]#hadoop fs -ls /user/cloudera/problem2/products/products
Found 5 items
-rw-r--r--   1 root cloudera          0 2018-03-13 08:26 /user/cloudera/problem2/products/products/_SUCCESS
-rw-r--r--   1 root cloudera      41419 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00000
-rw-r--r--   1 root cloudera      43660 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00001
-rw-r--r--   1 root cloudera      42195 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00002
-rw-r--r--   1 root cloudera      46719 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00003
//borramos de hdfs
[root@quickstart /]#hadoop fs -rm -f -r /user/cloudera/problem2/products/products
[root@quickstart /]#hadoop fs mkdir /user/cloudera/problem2/products
[root@quickstart /]#hadoop fs -mv /user/cloudera/products /user/cloudera/problem2/products/products
3)Cambie los permisos de todos los archivos en / user / cloudera / problem2 / products de modo 
que el propietario tenga permisos de lectura,escritura y ejecución, 
grupo tenga permisos de lectura y escritura, 
mientras que otros solo lean y ejecuten permisos

[root@quickstart /]# hadoop fs -chmod 765 /user/cloudera/problem2/products



4)
lea los datos en /user/cloudera/problem2/products y realice las siguientes operaciones utilizando 
a) dataframes api 
b) spark sql 
c) RDDs aggregateByKey method. 
Su solución debe tener tres conjuntos de pasos. Clasifique el conjunto de datos resultante por ID de categoría
4.1)filtre para que su RDD \ DF tenga productos cuyo precio sea inferior a 100 USD
4.2)en el conjunto de datos filtrado, averigüe el valor más alto en la columna product_price debajo de cada categoría
4.3)en el conjunto de datos filtrado también averigüe el total de productos en cada categoría
4.4)en el conjunto de datos filtrados también averigüe el precio promedio del producto en cada categoría
4.5)en el conjunto de datos filtrado también averigua el precio mínimo del producto en cada categoría








val productosArray = sc.textFile("/user/cloudera/problem2/products/products", 4).map(linea => {val arrayString = linea.split("|");(arrayString(0).toInt, arrayString(1).toInt, arrayString(2).toString, arrayString(3).toString, arrayString(4).toFloat, arrayString(5).toString)})


val productos = sc.textFile("/user/cloudera/problem2/products").map(p => (p.split("|")(1).toInt, p.split("|")(5).toFloat)).take(10).foreach(println)



sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table order_items   --as-textfile  --target-dir /user/cloudera/problema1 --fields-terminated-by ',' ; 
sc.textFile("/user/cloudera/problema1").take(100).foreach(println)

val rdd1=sc.textFile("/user/cloudera/problema1").map(reg => (reg.split(",")(1).toInt,reg.split(",")(4).toFloat)).take(10).foreach(println)
val orderItems = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat))
val rdd1 = orderItems.take(10).foreach(println)
//para orderId suma de subtotal.
val rdd2 = orderItems.reduceByKey((total, orderItemSubtotal) => total + orderItemSubtotal)
scala> rdd2.take(10).foreach(println)
ORDERID,SUM(SUBTOTAL)
(18624,199.99)                                                                  
(57436,1309.8501)
(54040,1049.8301)
(7608,39.99)
(18500,449.98)
(23556,299.95)
(23776,329.98)
(58592,699.85)
(29856,1029.92)
(32676,719.91003)
//el valor minimo de subtotal.
val rdd3 = orderItems.reduceByKey((total, orderItemSubtotal) => if(total< orderItemSubtotal)total else orderItemSubtotal);
//min, max subtotal
val orderItems4 = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, (orderItem.split(",")(4).toFloat, orderItem.split(",")(4).toFloat)));
orderItems3: org.apache.spark.rdd.RDD[(Int, Float, Float)] = MapPartitionsRDD[2] at map at <console>:27

val rdd4 = orderItems4.reduceByKey((total, orderItemSubtotal) => (if(total._1< orderItemSubtotal._1)total._1 else orderItemSubtotal._1,if(total._2> orderItemSubtotal._2)total._2 else orderItemSubtotal._2));

//min,max,count
val orderItems5 = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, (orderItem.split(",")(4).toFloat, orderItem.split(",")(4).toFloat,1)));

val rdd5 = orderItems5.reduceByKey((total, orderItemSubtotal) => (if(total._1< orderItemSubtotal._1)total._1 else orderItemSubtotal._1,if(total._2> orderItemSubtotal._2)total._2 else orderItemSubtotal._2, total._3 + orderItemSubtotal._3));

val ordItAgreg = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt,( orderItem.split(",")(4).toFloat,1)))
val rdd1 = orderItems.take(10).foreach(println)
//para orderId suma de subtotal.
//lo primero es la inicializacion de los campos a calcular sumatorio de campo float orderItemSubtotal  0.0 y count 0 de los items que es un entero
//Calcule los ingresos y la cantidad de elementos para cada pedido usando aggregateByKey
//(iTotal, oisubtotal) => (iTotal._1 + oisubtotal._1, iTotal._2 + oisubtotal._2) se ejecuta en cada particion agregaciones parciales por key en la particion
//(fTotal, iTotal) => (fTotal._1 + iTotal._1, fTotal._2 + iTotal._2) se ejecuta en el driver mergeando los valores parciales
val rdd2 = orderItems.aggregateByKey((0.0, 0))((iTotal, oisubtotal) => (iTotal._1 + oisubtotal._1, iTotal._2 + oisubtotal._2),(fTotal, iTotal) => (fTotal._1 + iTotal._1, fTotal._2 + iTotal._2))

((total, orderItemSubtotal) => total + orderItemSubtotal)
scala> rdd2.take(10).foreach(println)

val ordItAgreg2 = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toInt))
	

val rdd3 = ordItAgreg2.aggregateByKey((0.0))(math.min(_,_), math.min(_,_))
val rdd2 = ordItAgreg2.aggregateByKey((0.0))((iTotal, oisubtotal) 
=> ( if(iTotal._1<oisubtotal._1)iTotal._1 else oisubtotal._1) => (if(fTotal._1 < iTotal._1)fTotal._1 else iTotal._1, if(fTotal._2 > iTotal._2)fTotal._2 else iTotal._2 ));



select Host,User,Password from Users;


#problema 3 Import all tables

# CUANDO SE QUIERE IMPORTAR TODAS TABLAS NO SE ESPICIFICA NINGUNA, --WAREHOUSE-DIR: ESPECIFICA LA RUTA DE HDFS EN VEZ DE TARGET-DIR: RUTA DE HDFS , EL PARAMETRO -M 1 ES PARA QUE LAS TAREAS SE EJECUTEN EN UN SOLO HILO PARA EVITAR LAS DEPENDENCIAS
sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username retail_dba  --password cloudera  --warehouse-dir /user/hive/warehouse/retail_stage.db  --compress --compression-codec snappy --as-avrodatafile  -m 1 ; 


        [root@quickstart /]# hadoop fs -ls -R /user/hive/warehouse/
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/categories
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/categories/_SUCCESS
	-rw-r--r--   1 root supergroup       1378 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/categories/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/customers
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/customers/_SUCCESS
	-rw-r--r--   1 root supergroup     469715 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/customers/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:29 /user/hive/warehouse/retail_stage.db/departments
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:29 /user/hive/warehouse/retail_stage.db/departments/_SUCCESS
	-rw-r--r--   1 root supergroup        475 2018-03-15 10:29 /user/hive/warehouse/retail_stage.db/departments/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/order_items
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/order_items/_SUCCESS
	-rw-r--r--   1 root supergroup    1526314 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/order_items/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/orders
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/orders/_SUCCESS
	-rw-r--r--   1 root supergroup     660183 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/orders/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db/products
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db/products/_SUCCESS
	-rw-r--r--   1 root supergroup      53592 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db/products/part-m-00000.avro

#TRAEMOS LOS FICHEROS DE HDFS A LOCAL
#CUANDO SE QUIERE CARGA DE MYSQL A UNA TABLA DE HIVE SE DEBE
1.- CARGAR CON SQOOP DESDE MYSQL Y SE PUEDE GUARDAR COMO HDFS
2.- QUITAR EL FORMATO AVRO Y COPIARLO DE HDFS A LOCAL MOVER DE LOCAL(SIN FORMATO AVRO) A HFDS DE NUEVO
3.- DESDE HIVE CARGAMOS EL FICHERO ALMACENADO EN HDFS Y LO GUARDAMOS EN UNA TABLA EXTERNA CON FORMATO AVRO
[root@quickstart /]# hadoop fs -get /user/hive/warehouse/retail_stage.db/order_items/part-m-00000.avro
#CON LA HERRAMIENTA AVRO_TOOL DE CLOUDERA PASAMOS DE AVRO A OTRO FORMATO
avro-tools getschema part-m-00000.avro > orders.avsc
[root@quickstart /]# ls -l |grep avsc
-rw-r--r--   1 root root     594 Mar 15 10:27 categories.avsc
-rw-r--r--   1 root root    1509 Mar 15 10:28 customers.avsc
-rw-r--r--   1 root root     440 Mar 15 10:29 departments.avsc
-rw-r--r--   1 root root    1099 Mar 15 10:29 order_items.avsc
-rw-r--r--   1 root root    1100 Mar 15 11:52 orders.avsc
-rw-r--r--   1 root root    1041 Mar 15 10:30 products.avsc

hadoop fs -mkdir /user/hive/schemas
hadoop fs -put orders.avsc /user/hive/schemas/order =>ES IGUAL hadoop fs -copyFromLocal orders.avsc /user/hive/schemas/order

create external table orders_sqoop STORED AS AVRO LOCATION '/user/hive/warehouse/retail_stage.db/orders' TBLPROPERTIES ('avro.schema.url'='/user/hive/schemas/order/orders.avsc')
#query table in impala that shows all orders belonging to a certain day. This day is when the most orders were placed. select data from order_sqoop.
#CREAR UN QUERY SOBRE LA TABLA order_sqoop DE LOS DIAS EN LOS CUALES SE HA REALIZADP LA MAYORIA DE LOS PEDIDOS.
[root@quickstart /]# hadoop fs -cat /user/hive/schemas/order
{
  "type" : "record",
  "name" : "order_items",
  "doc" : "Sqoop import of order_items",
  "fields" : [ {
    "name" : "order_item_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_id",
    "sqlType" : "4"
  }, {
    "name" : "order_item_order_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_order_id",
    "sqlType" : "4"
  }, {
    "name" : "order_item_product_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_product_id",
    "sqlType" : "4"
  }, {
    "name" : "order_item_quantity",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_quantity",
    "sqlType" : "-6"
  }, {
    "name" : "order_item_subtotal",
    "type" : [ "null", "float" ],
    "default" : null,
    "columnName" : "order_item_subtotal",
    "sqlType" : "7"
  }, {
    "name" : "order_item_product_price",
    "type" : [ "null", "float" ],
    "default" : null,
    "columnName" : "order_item_product_price",
    "sqlType" : "7"
  } ],
  "tableName" : "order_items"
}
Hive> create external table orders_sqoop
    > STORED AS AVRO
    > LOCATION '/user/hive/warehouse/retail_stage.db/orders'
    > TBLPROPERTIES ('avro.schema.url'='/user/hive/schemas/order');
OK
Time taken: 2.02 seconds
#QUERY EN HIVE QUE RECOGE 5 REGISTROS DE TODOS LOS PEDIDOS QUE QUE SE REALIZARON EL DIA QUE MAS PEDIDOS SE HICIERON.
hive> select * from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5;
# HACER LO MISMO DE ANTES EN IMPALA.
impala-shell
[quickstart.cloudera:21000] > invalidate metadata ;

[quickstart.cloudera:21000] > select * from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5;
Query: select * from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5
+----------+---------------+-------------------+-----------------+
| order_id | order_date    | order_customer_id | order_status    |
+----------+---------------+-------------------+-----------------+ 
| 16078    | 1383436800000 | 65                | PENDING_PAYMENT |
| 16077    | 1383436800000 | 3407              | CLOSED          |
| 16076    | 1383436800000 | 6833              | CANCELED        |
| 16075    | 1383436800000 | 7051              | SUSPECTED_FRAUD |
| 16074    | 1383436800000 | 11930             | PENDING_PAYMENT |
+----------+---------------+-------------------+-----------------+

select oq.*,from_unixtime(oq.order_date/(365*24*60*60*1000)) from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5;

#Import orders table from mysql as text file to the destination /user/cloudera/problem5/text. 
#Fields should be terminated by a tab character (“\t”) character and lines should be terminated by new line character (“\n”).

sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problem5/text --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n';
#Import orders table from mysql  into hdfs to the destination /user/cloudera/problem5/avro. File should be stored as avro file.
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problem5/avro --as-avrodatafile ;
#Import orders table from mysql  into hdfs  to folders /user/cloudera/problem5/parquet. File should be stored as parquet file SQOOP_PARQUET.

sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problem5/parquet --as-parquetfile ;
[root@quickstart /]# hadoop fs -ls -R /user/cloudera/problem5
drwxr-xr-x   - root cloudera          0 2018-03-19 09:47 /user/cloudera/problem5/avro
-rw-r--r--   1 root cloudera          0 2018-03-19 09:47 /user/cloudera/problem5/avro/_SUCCESS
-rw-r--r--   1 root cloudera     439146 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00000.avro
-rw-r--r--   1 root cloudera     447726 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00001.avro
-rw-r--r--   1 root cloudera     446959 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00002.avro
-rw-r--r--   1 root cloudera     447606 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00003.avro
drwxr-xr-x   - root cloudera          0 2018-03-19 10:00 /user/cloudera/problem5/parquet
drwxr-xr-x   - root cloudera          0 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata
-rw-r--r--   1 root cloudera        184 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/descriptor.properties
-rw-r--r--   1 root cloudera        707 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/schema.avsc
drwxr-xr-x   - root cloudera          0 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/schemas
-rw-r--r--   1 root cloudera        707 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/schemas/1.avsc
drwxr-xr-x   - root cloudera          0 2018-03-19 10:00 /user/cloudera/problem5/parquet/.signals
-rw-r--r--   1 root cloudera          0 2018-03-19 10:00 /user/cloudera/problem5/parquet/.signals/unbounded
-rw-r--r--   1 root cloudera     147264 2018-03-19 10:00 /user/cloudera/problem5/parquet/18e8ba61-3ac0-4ab7-9784-d01e42b11d30.parquet
-rw-r--r--   1 root cloudera     147312 2018-03-19 10:00 /user/cloudera/problem5/parquet/2c528d70-cf06-42eb-9e1e-419b79ba6ee7.parquet
-rw-r--r--   1 root cloudera     147465 2018-03-19 10:00 /user/cloudera/problem5/parquet/5479e2f2-42b4-4ad8-b8c8-53f8a8c554f3.parquet
-rw-r--r--   1 root cloudera     151743 2018-03-19 10:00 /user/cloudera/problem5/parquet/6590cdab-4245-402a-94d7-c778527c9aab.parquet
drwxr-xr-x   - root cloudera          0 2018-03-19 09:44 /user/cloudera/problem5/text
-rw-r--r--   1 root cloudera          0 2018-03-19 09:44 /user/cloudera/problem5/text/_SUCCESS
-rw-r--r--   1 root cloudera     741614 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00000
-rw-r--r--   1 root cloudera     753022 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00001
-rw-r--r--   1 root cloudera     752368 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00002
-rw-r--r--   1 root cloudera     752940 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00003

#Transform/Convert data-files at /user/cloudera/problem5/avro and store the converted file at the following locations and file formats
# En un ide
  val conf = new SparkConf().setAppName("miApp").setMaster("local[2]")
  val sc=new SparkContext(conf) 
  val sqlContext = new org.apache.spark.sql.SQLContext(sc) 
#  1.-save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem5/parquet-snappy-compress
  val orders = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/format-avro")
orders.write.option("spark.io.compression.codec", "snappy").parquet("/user/cloudera/problem5/parquet-snappy-compress")
#  2.-save the data to hdfs using gzip compression as text file at /user/cloudera/problem5/text-gzip-compress
  val orderTextGzip=orders.map(x => x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)+"\t")
  orderTextGzip.saveAsTextFile("/user/cloudera/problem5/text-gzip-compress", classOf[org.apache.hadoop.io.compress.GzipCodec])
#  3.-save the data to hdfs using no compression as sequence file at /user/cloudera/problem5/sequence
  orderTextGzip.saveAsTextFile("/user/cloudera/problem5/text")
#  4.-save the data to hdfs using snappy compression as text file at /user/cloudera/problem5/text-snappy-compress
orderTextGzip.saveAsTextFile("/user/cloudera/problem5/text-snappy-compress1", classOf[org.apache.hadoop.io.compress.SnappyCodec])



#Transform/Convert data-files at /user/cloudera/problem5/parquet-snappy-compress and store the converted file at the following locations and file formats
# 1.- save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress

   val ordersParquetSinCompress = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress1")
   #DESACTIVAR COMPRESION
   sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
   #ESCRIBIR 
   val  ordersParquetSinCompressDF=ordersParquetSinCompress.write.parquet("/user/cloudera/problem5/parquet-no-compress")
   #ACTIVAR COMPRESION
   sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
# 2.- save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy
ordersParquetSinCompress.write.format("com.databricks.spark.avro").save("/user/cloudera/problem5/avro-snappy")

# Transform/Convert data-files at /user/cloudera/problem5/avro-snappy and store the converted file at the following locations and file formats
# 1.- save the data to hdfs using no compression as json file at /user/cloudera/problem5/json-no-compress
   val rddAvro=sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/avro-snappy")
   sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
   rddAvro.write.json("/user/cloudera/problem5/json-no-compress")
   scala> rddAvro.write.json("/user/cloudera/problem5/json-no-compress") 
	[Stage 0:>                                                          
	([Stage 0:>                                                         
	([Stage 0:==============>                                            
	([Stage 0:=============================>                             
	([Stage 0:===========================================================(
   sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
# 2.- save the data to hdfs using gzip compression as json file at /user/cloudera/problem5/json-gzip
   val rddAvro=sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/avro-snappy")
   rddAvro.write.option("spark.io.compression.codec", "gzip").json("/user/cloudera/problem5/json-gzip")
   scala> rddAvro.write.option("spark.io.compression.codec", "gzip").json("/user/cloudera/problem5/json-gzip")
    [Stage 1:=============================>
    
#Transform/Convert data-files at  /user/cloudera/problem5/json-gzip and store the converted file at the following locations and file formats
# 1.-save the data to as comma separated text using gzip compression at   /user/cloudera/problem5/csv-gzip
scala> var jsonData = sqlContext.read.json("/user/cloudera/problem5/json-gzip");
jsonData: org.apache.spark.sql.DataFrame = [order_customer_id: bigint, order_date: bigint, order_id: bigint, order_status: string]
scala> jsonData.map(x=>x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/cloudera/problem5/csv-gzip", classOf[org.apache.hadoop.io.compress.GzipCodec])

#Using spark access data at /user/cloudera/problem5/sequence and stored it back to hdfs using no compression as ORC file to HDFS to destination /user/cloudera/problem5/orc
 sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db"   --password cloudera   --username retail_dba   --table orders   --as-avrodatafile   --target-dir /user/cloudera/problem5/avro1   -m 1
 import com.databricks.spark.avro
 var dataFile = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/avro1");
 dataFile.map(x=> (x(0).toString,x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3))).saveAsSequenceFile("/user/cloudera/problem5/sequence");
 scala> var seqData = sc.sequenceFile("/user/cloudera/problem5/sequence/", classOf[org.apache.hadoop.io.Text], classOf[org.apache.hadoop.io.Text]);
 seqData: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.Text, org.apache.hadoop.io.Text)] = /user/cloudera/problem5/sequence/ HadoopRDD[53] at sequenceFile at <console>:28
 
 seqData.map(x => { var d = x._2.toString.split("\t"); (d(0),d(1),d(2),d(3)) }).toDF().write.orc("/user/cloudera/problem5/orc");


#SQOOP JOB

sqoop job --create incjob -- import --connect jdbc:mysql://localhost:3306/test --driver com.mysql.jdbc.Driver --username it1 --password hadoop --table st1 --incremental lastmodified -check-column ts --target-dir sqin -m 1 --merge-key id


#Problem Scenario 3: Please accomplish following activities. 
#1.  Create a table region with following structure. However, underline file format should be parquet in HDFS.
#	r_regionkey smallint,
#	r_name      string,
#	r_comment   string,
#	r_nations   array<struct<n_nationkey:smallint,n_name:string,n_comment:string>> 

hive> Create external table if not exists Region
     (
     r_regionkey smallint,
     r_name string,
     r_comment string,
     r_nations array <struct<n_nationkey:smallint,n_name:string,n_comment:string>> 
     )ROW FORMAT DELIMITED
     FIELDS TERMINATED  BY '|'
     COLLECTION ITEMS TERMINATED BY ','
     STORED AS PARQUET;
#2. Once table is created , load data in this table from region.csv
#Data for region table : region.csv
r_regionkey|r_name|r_comment|r_nations &amp;lt;n_nationkey,n_name,n_comment>
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for HadoopExam.com|1,United States,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for Training4Exam.com|2,Canada,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|3,Cuba,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for Training4Exam.com|17,Costa Rica,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|24,Panama,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|8,India,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|9,China,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|12,Japan,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|18,Russia,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|21,Israel,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|6,Austria,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|7,Bulgaria,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|19,Belgium,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|22,Croatia,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|23,Denmark,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|4,Saudi Arabia,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|10,Yemen,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|11,Oman,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|13,Kuwait,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|20,Qatar,Reference site http://www.QuickTechie.com#





[root@quickstart /]#echo "r_regionkey|r_name|r_comment|r_nations &amp;lt;n_nationkey,n_name,n_comment>
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for HadoopExam.com|1,United States,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for Training4Exam.com|2,Canada,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|3,Cuba,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for Training4Exam.com|17,Costa Rica,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|24,Panama,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|8,India,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|9,China,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|12,Japan,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|18,Russia,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|21,Israel,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|6,Austria,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|7,Bulgaria,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|19,Belgium,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|22,Croatia,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|23,Denmark,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|4,Saudi Arabia,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|10,Yemen,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|11,Oman,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|13,Kuwait,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|20,Qatar,Reference site http://www.QuickTechie.com#
" > Region.csv

[root@quickstart /]# hadoop fs -mkdir /user/cloudera/hadoop
[root@quickstart /]# hadoop fs -mkdir /user/cloudera/hadoop/exam
[root@quickstart /]# awk 'NR != 1 {print}' Region.csv | hdfs dfs -put - /user/cloudera/hadoop/exam/region.csv

[root@quickstart /]# hadoop fs -cat /user/cloudera/hadoop/exam/region.csv
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
[root@quickstaawk 'NR != 1 {print}' Region.csv | hdfs dfs -put -s -cat  /user/cloudera/hadoop/exam/region.c
[root@quickstart /]#  hdfs dfs -cat /user/cloudera/hadoop/exam/region.csv
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for HadoopExam.com|1,United States,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for Training4Exam.com|2,Canada,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|3,Cuba,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for Training4Exam.com|17,Costa Rica,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|24,Panama,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|8,India,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|9,China,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|12,Japan,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|18,Russia,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|21,Israel,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|6,Austria,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|7,Bulgaria,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|19,Belgium,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|22,Croatia,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|23,Denmark,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|4,Saudi Arabia,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|10,Yemen,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|11,Oman,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|13,Kuwait,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|20,Qatar,Reference site http://www.QuickTechie.com#



scala> val textFile = sc.textFile("hdfs:///user/cloudera/hadoop/exam/region.csv");

textFile: org.apache.spark.rdd.RDD[String] = hdfs:///user/cloudera/hadoop/exam/region.csv MapPartitionsRDD[1] at textFile at <console>:27

scala> textFile.take(5).foreach(println)
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
scala> import sqlContext.implicits._
scala> val df = textFile.toDF("line")
scala> df.write.parquet("hdfs:///user/cloudera/hadoop/exam/parquetRegion");


hive> load data inpath '/user/cloudera/hadoop/exam/parquetRegion' into table region;

hive> create table tempresgion(data string);
hive> load data inpath '/user/cloudera/hadoop/exam/region.csv' into table tempresgion;
hive> select split(data,'\\|')[0] r_regionkey,
    > split(data,'\\|')[1] r_name,
    > split(data,'\\|')[2] r_commet,
    > split(split(data,'\\|')[3],",")[0] n_nationKey,
    > split(split(data,'\\|')[3],",")[1] n_name,
    > split(split(data,'\\|')[3],",")[2] n_comment 
    > from 
    > tempresgion limit 5;
OK
1	AFRICA	Good Business Region for HadoopExam.com	0	Cameroon	Reference site http://www.QuickTechie.com
1	AFRICA	Good Business Region for Training4Exam.com	5	Egypt	Reference site http://www.HadoopExam.com
1	AFRICA	Good Business Region for HadoopExam.com	14	Namibia	Reference site http://www.QuickTechie.com
1	AFRICA	Good Business Region for Training4Exam.com	15	Zimbabwe	Reference site http://www.HadoopExam.com
1	AFRICA	Good Business Region for HadoopExam.com	16	Uganda	Reference site http://www.QuickTechie.com
Time taken: 0.127 seconds, Fetched: 5 row(s)

hive> insert overwrite table region 
    > select split(data,'\\|')[0] r_regionkey,
    > split(data,'\\|')[1] r_name,
    > split(data,'\\|')[2] r_commet,
    > array(named_struct("n_nationkey",cast(split(split(data,'\\|')[3],",")[0] as smallint),"n_name",split(split(data,'\\|')[3],",")[1],"n_comment",split(split(data,'\\|')[3],",")[2] )) from tempresgion;
Query ID = root_20180405104747_79e1b18a-be6b-48f1-b2a3-cb15c2658c99
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1522922964151_0006, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1522922964151_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1522922964151_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-04-05 14:12:59,110 Stage-1 map = 0%,  reduce = 0%
2018-04-05 14:13:05,401 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.28 sec
MapReduce Total cumulative CPU time: 2 seconds 280 msec
Ended Job = job_1522922964151_0006
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/region/.hive-staging_hive_2018-04-05_14-12-51_476_533907710585064828-1/-ext-10000
Loading data to table default.region
Table default.region stats: [numFiles=1, numRows=26, totalSize=2432, rawDataSize=104]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.28 sec   HDFS Read: 7580 HDFS Write: 2503 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 280 msec
OK
Time taken: 15.416 seconds
Time taken: 0.131 seconds, Fetched: 26 row(s)
hive> select * from region limit 10;
OK
1	AFRICA	Good Business Region for HadoopExam.com	[{"n_nationkey":0,"n_name":"Cameroon","n_comment":"Reference site http://www.QuickTechie.com"}]
1	AFRICA	Good Business Region for Training4Exam.com	[{"n_nationkey":5,"n_name":"Egypt","n_comment":"Reference site http://www.HadoopExam.com"}]
1	AFRICA	Good Business Region for HadoopExam.com	[{"n_nationkey":14,"n_name":"Namibia","n_comment":"Reference site http://www.QuickTechie.com"}]
1	AFRICA	Good Business Region for Training4Exam.com	[{"n_nationkey":15,"n_name":"Zimbabwe","n_comment":"Reference site http://www.HadoopExam.com"}]
1	AFRICA	Good Business Region for HadoopExam.com	[{"n_nationkey":16,"n_name":"Uganda","n_comment":"Reference site http://www.QuickTechie.com"}]
2	AMERICA	Average Business Region for HadoopExam.com	[{"n_nationkey":1,"n_name":"United States","n_comment":"Reference site http://www.HadoopExam.com"}]
2	AMERICA	Average Business Region for Training4Exam.com	[{"n_nationkey":2,"n_name":"Canada","n_comment":"Reference site http://www.HadoopExam.com"}]
2	AMERICA	Average Business Region for HadoopExam.com	[{"n_nationkey":3,"n_name":"Cuba","n_comment":"Reference site http://www.QuickTechie.com"}]
2	AMERICA	Average Business Region for Training4Exam.com	[{"n_nationkey":17,"n_name":"Costa Rica","n_comment":"Reference site http://www.HadoopExam.com"}]
2	AMERICA	Average Business Region for HadoopExam.com	[{"n_nationkey":24,"n_name":"Panama","n_comment":"Reference site http://www.HadoopExam.com"}]
Time taken: 0.115 seconds, Fetched: 10 row(s)
 

#Crear un tabla que contenga valores numerícos que la columna se llame value y otra columna que se llame property 
y tenga el valor es par(even) ó impar(odd).  

En 192.168.2.137 => sudo apt-get intall openssh-server; nmap localhost => abrir puerto 22
En 172.17.0.2 => sudo apt-get intall openssh-server; service sshd status; service sshd start.

scp -v cristina.cano@192.168.2.137:/home/cristina.cano/hadoopexamintFile.csv root@172.17.0.2:/home/cloudera

hdfs dfs -mkdir /user/cloudera/hadoopexam

hdfs dfs -put /home/cloudera/hadoopexamintFile.csv /user/cloudera/hadoopexam
hive> create external table hadoopexamint1 (value int, property string) row format delimited fields terminated by ',' stored as textfile  ;
hive> load data inpath '/user/cloudera/hadoopexam/hadoopexamintFile.csv' into table hadoopexamint;
















