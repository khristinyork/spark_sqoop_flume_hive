Install cloudera
####################

Cloudera QuickStart Docker Image
Cloudera QuickStart VMs and this Docker image are single-node deployments of Cloudera's 100% open-source distribution including Apache Hadoop, and Cloudera Manager. They are ideal environments for learning about Hadoop, trying out new ideas, testing and demoing your application.

Introduction to Docker
Docker is different from other platforms you may have worked with, because it uses Linux containers. Most "virtual machines" work by isolating or simulating access to the host's hardware so an entire guest operating system can run on it. Linux containers, however, work by partitioning the resources of the host operating system: they have their own view of the filesystem and other resources, but they are running on the same kernel. This is similar to BSD jails or Solaris zones. Docker provides tooling, a packaging format, and infrastructure around Linux containers and related technologies.

Docker is well supported in several recent Linux distributions. For instance, on Ubuntu 14.04, it can be installed as follows:

sudo apt-get install docker.io
Importing the Cloudera QuickStart Image
You can import the Cloudera QuickStart image from Docker Hub:

docker pull cloudera/quickstart:latest
Running a Cloudera QuickStart Container
To run a container using the image, you must know the name or hash of the image. If you followed the Importing instructions above, the name could be cloudera/quickstart:latest (or something else if you have multiple versions downloaded). The hash is also printed in the terminal when you import, or you can look up the hashes of all imported images with:

docker images
Once you know the name or hash of the image, you can run it:

docker run --hostname=quickstart.cloudera --privileged=true -t -i [OPTIONS] [IMAGE] /usr/bin/docker-quickstart
Explanation for required flags and other options are in the following table:

--hostname=quickstart.cloudera    Required: pseudo-distributed configuration assumes this hostname
--privileged=true                 Required: for HBase, MySQL-backed Hive metastore, Hue, Oozie, Sentry, and Cloudera Manager, and possibly others
-t                                Required: once services are started, a Bash shell takes over and will die without this
-i                                Required: if you want to use the terminal, either immediately or attach later
-p 8888                           Recommended: maps the Hue port in the guest to another port on the host
-p [PORT]                         Optional: map any other ports (e.g. 7180 for Cloudera Manager, 80 for a guided tutorial)
-d                                Optional: runs the container in the background
/usr/bin/docker-quickstart is provided as a convenience to start all CDH services, then run a Bash shell. You can directly run /bin/bash instead if you wish to start services manually.

See "Networking" for details about port mapping.

Connecting To The Shell
If you do not pass the -d flag to docker run, your terminal will automatically attach to that of the container.

A container will die when you exit the shell, but you can disconnect and leave the container running by hitting Ctrl+P -> Ctrl+Q.

If you have disconnected from the shell or did pass the -d flag, you can connect to the shell later with:

docker attach [CONTAINER HASH]
You can look up the hashes of running containers with:

docker ps
When attaching to a container, you may need to hit to see the shell prompt. To disconnect from the terminal without the container exiting, hit Ctrl+P -> Ctrl+Q.

Networking
To make a port accessible outside the container, pass the -p <port> flag. Docker will map this port to another one on the host system. You can look up the interface to which it binds and the port number it maps to with:

docker port [CONTAINER HASH] [GUEST PORT]
If you're going to interact with the Cloudera QuickStart image from other systems, you should make sure quickstart.cloudera resolves to the IP address of the machine where the image is running. You may also want to set up port-forwarding so that the port you would normally connect to on a real cluster, is mapped to the corresponding port

Be aware that when you are mapping ports like this, services are not aware, and may give you links or other references to specific ports that are no longer resolvable on your client.

Other Notes
Docker containers will not have any permanent storage unless you set it up. When the container is killed, any data not found in the image will be lost.

Remember that Cloudera's stack is designed to run on a distributed cluster. Pausing this docker image is like pausing an entire data center: some services may shut down because from their perspective it's been a long time since they were able to communicate with the rest of the cluster.

Cloudera Manager is not started by default. To see options for starting it, run:

/home/cloudera/cloudera-manager
/home/cloudera/parcels can then be used if you wish to migrate from a package-based install to a parcel-based install. /home/cloudera/kerberos can be used to install and start Kerberos.

See Cloudera's documentation and Cloudera's website for other information, including the license agreement associated with this image. Cloudera QuickStart is not intended or supported for use in production.
#CLOUDERA EJERCICIOS.


sqlContext.sql("SET spark.sql.parquet.compression.codec=snappy")

#ARRANCAR DOCKER DE CLOUDERA
docker run --hostname=quickstart.cloudera --privileged=true -t -i -v /home/docker-clodera:/src --publish-all=true -p 8088:8088  -p 8888:8888 -p 50090:50090 -p 50070:50070 -p 50075:50075 -p 8042:8042 -p 60030:60030 -p 25000:25000 -p 25010:25010 -p 18088:18088 -p 8983:8983 -p 11000:11000 cloudera/quickstart /usr/bin/docker-quickstart
docker ps => te muestra el containerid =f832867cde6c
docker exec -it f832867cde6c /bin/bash


PARA SABER EL TAMAÑO DE UN RDD EN MEMORIA
import org.apache.spark.util.SizeEstimator
println(SizeEstimator.estimate(rdd1));


1.-Planteamiento del problema:Usando sqoop, importe la tabla de órdenes en hdfs a folders 
 / user / cloudera / problem1 / orders . 
 El archivo debe cargarse como archivo Avro y usar compresión rápida.
 
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --target-dir /user/cloudera/problema1/orders --as-avrodatafile;
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problema1/orders_nocopression_snnapy --as-avrodatafile;
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --target-dir /user/cloudera/problema1/ordersTextFile --as-textfile;
sqoop-import  --connect jdbc:mysql://quickstart.cloudera:3306/retail_db  --username root --password cloudera --table order_items --compress  --compression-codec org.apache.hadoop.io.compress.SnappyCodec  --target-dir /user/cloudera/problema1/order-items --as-avrodatafile;
hadoop fs -ls -R /user/cloudera/problema1drwxr-xr-x   - root cloudera          0 2018-03-12 07:40 /user/cloudera/problema1/orders
-rw-r--r--   1 root cloudera          0 2018-03-12 07:40 /user/cloudera/problema1/orders/_SUCCESS
-rw-r--r--   1 root cloudera     164082 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00000.avro
-rw-r--r--   1 root cloudera     164153 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00001.avro
-rw-r--r--   1 root cloudera     164269 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00002.avro
-rw-r--r--   1 root cloudera     169328 2018-03-12 07:40 /user/cloudera/problema1/orders/part-m-00003.avro
drwxr-xr-x   - root cloudera          0 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile
-rw-r--r--   1 root cloudera          0 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/_SUCCESS
-rw-r--r--   1 root cloudera     218873 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00000.snappy
-rw-r--r--   1 root cloudera     218610 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00001.snappy
-rw-r--r--   1 root cloudera     220683 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00002.snappy
-rw-r--r--   1 root cloudera     233412 2018-03-12 07:59 /user/cloudera/problema1/ordersTextFile/part-m-00003.snappy
drwxr-xr-x   - root cloudera          0 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy
-rw-r--r--   1 root cloudera          0 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/_SUCCESS
-rw-r--r--   1 root cloudera     439146 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00000.avro
-rw-r--r--   1 root cloudera     447726 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00001.avro
-rw-r--r--   1 root cloudera     446959 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00002.avro
-rw-r--r--   1 root cloudera     447606 2018-03-12 07:50 /user/cloudera/problema1/orders_nocopression_snnapy/part-m-00003.avro



select table_schema "DATABASE",TABLE_NAME "tableName",convert(sum(data_length+index_length)/1048576,decimal(6,2)) "SIZE (MB)" from  information_schema.tables where   table_schema="retail_db" group by  table_schema,TABLE_NAME;

2. Problema 2  Utilizando Sqoop, importar la tabla “order_items” en el directorio de HDFS /user/cloudera/problema1/order_items. El tipo de fichero debe de ser Avro y el tipo de compresión Snappy. 

sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table order_items  --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec  --target-dir /user/cloudera/problema1/order-items  --as-avrodatafile; 
 
 
 
 2.2.- Problema 4  Vamos a generar resultados intermedios con el siguiente formato de columnas:
 ● order_date (fecha del pedido)  ● order_status (estado del pedido)  ● total_orders (número total de pedidos) ● total_amount (suma total ) 
 
El resultado debería ordenarse por: ● order_date  en orden descendente ● order_status  ascendente  ● total_amount  en descendente ● total_orders  en ascendente. 
 
El objetivo es encontrar el número total de orders y la cantidad total (amount) por estado y por día. 
 
Las agregaciones deberían realizarse utilizando los métodos descritos más abajo: a) Utilizando la API de DataFrames - order_date debería tener el formato YYYY-MM-DD b) Utilizando Spark SQL - order_date debería tener el formato YYYY-MM-DD c) Utilizando la función CombineByKey en los RDDs. No es necesario formatear el campo order_date o total_amount. 
 
Lo primero de todos vamos a unir los dos conjuntos de datos para tener un dataframe con todas las columnas necesarias. 


[root@quickstart /]# spark-shell

scala>
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val orders = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problema1/orders")
val order_items = sqlContext.read.avro("/user/cloudera/problema1/order-items") 
val order_items = sqlContext.read.avro("/user/cloudera/problema1/order-items") 

Crear una tabla en mysql 
#mysql -uroot -pcloudera
mysql>create table retail_db.result(ordered_date varchar(255) not null, order_status varchar(255) not null,total_orders int not null,total_amount numeric not null );

//SQOOP importar un fichero de HDFS a una tabla de mysql
sqoop export --table result --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba --password cloudera --export-dir "/user/cloudera/problem1/result4a-csv" --columns "order_date,order_status,total_amount,total_orders"
//SQOOP exportar de una tabla de mysql a hdfs
sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table order_items  --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec  --target-dir /user/cloudera/problema1/order-items  --as-avrodatafile;  

//EJERCICIO 2 
1)utilizando SQOOP copia los datos de la tabla productos a un directorio de HDFS separando los campos por "|"

sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table products  --as-textfile  --target-dir /user/cloudera/products --fields-terminated-by '|'  ;  



Table 5. Output line formatting arguments:

Argument	Description
--enclosed-by <char>	          Sets a required field enclosing character
--escaped-by <char>	              Sets the escape character
--fields-terminated-by <char>	  Sets the field separator character
--lines-terminated-by <char>	  Sets the end-of-line character
--mysql-delimiters	              Uses MySQL’s default delimiter set: fields: , lines: \n escaped-by: \ optionally-enclosed-by: '
--optionally-enclosed-by <char>	  Sets a field enclosing character

Delimiters may be specified as:

a character (--fields-terminated-by X)
an escape character (--fields-terminated-by \t). Supported escape characters are:

\b (backspace)
\n (newline)
\r (carriage return)
\t (tab)
\" (double-quote)
\\' (single-quote)
\\ (backslash)
\0 (NUL) - This will insert NUL characters between fields or lines, or will disable enclosing/escaping if used for one of the --enclosed-by, --optionally-enclosed-by, or --escaped-by arguments.


2)move all the files from /user/cloudera/products folder to /user/cloudera/problem2/products folder
[root@quickstart /]# hadoop fs -mkdir /user/cloudera/problem2
[root@quickstart /]# hadoop fs -mkdir /user/cloudera/problem2/products
[root@quickstart /]# hadoop fs -ls /user/cloudera/problem2/products

[root@quickstart /]#hadoop fs -ls /user/cloudera/problem2/products/products
Found 5 items
-rw-r--r--   1 root cloudera          0 2018-03-13 08:26 /user/cloudera/problem2/products/products/_SUCCESS
-rw-r--r--   1 root cloudera      41419 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00000
-rw-r--r--   1 root cloudera      43660 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00001
-rw-r--r--   1 root cloudera      42195 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00002
-rw-r--r--   1 root cloudera      46719 2018-03-13 08:26 /user/cloudera/problem2/products/products/part-m-00003
//borramos de hdfs
[root@quickstart /]#hadoop fs -rm -f -r /user/cloudera/problem2/products/products
[root@quickstart /]#hadoop fs mkdir /user/cloudera/problem2/products
[root@quickstart /]#hadoop fs -mv /user/cloudera/products /user/cloudera/problem2/products/products
3)Cambie los permisos de todos los archivos en / user / cloudera / problem2 / products de modo 
que el propietario tenga permisos de lectura,escritura y ejecución, 
grupo tenga permisos de lectura y escritura, 
mientras que otros solo lean y ejecuten permisos

[root@quickstart /]# hadoop fs -chmod 765 /user/cloudera/problem2/products



4)
lea los datos en /user/cloudera/problem2/products y realice las siguientes operaciones utilizando 
a) dataframes api 
b) spark sql 
c) RDDs aggregateByKey method. 
Su solución debe tener tres conjuntos de pasos. Clasifique el conjunto de datos resultante por ID de categoría
4.1)filtre para que su RDD \ DF tenga productos cuyo precio sea inferior a 100 USD
4.2)en el conjunto de datos filtrado, averigüe el valor más alto en la columna product_price debajo de cada categoría
4.3)en el conjunto de datos filtrado también averigüe el total de productos en cada categoría
4.4)en el conjunto de datos filtrados también averigüe el precio promedio del producto en cada categoría
4.5)en el conjunto de datos filtrado también averigua el precio mínimo del producto en cada categoría








val productosArray = sc.textFile("/user/cloudera/problem2/products/products", 4).map(linea => {val arrayString = linea.split("|");(arrayString(0).toInt, arrayString(1).toInt, arrayString(2).toString, arrayString(3).toString, arrayString(4).toFloat, arrayString(5).toString)})


val productos = sc.textFile("/user/cloudera/problem2/products").map(p => (p.split("|")(1).toInt, p.split("|")(5).toFloat)).take(10).foreach(println)



sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera  --table order_items   --as-textfile  --target-dir /user/cloudera/problema1 --fields-terminated-by ',' ; 
sc.textFile("/user/cloudera/problema1").take(100).foreach(println)

val rdd1=sc.textFile("/user/cloudera/problema1").map(reg => (reg.split(",")(1).toInt,reg.split(",")(4).toFloat)).take(10).foreach(println)
val orderItems = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat))
val rdd1 = orderItems.take(10).foreach(println)
//para orderId suma de subtotal.
val rdd2 = orderItems.reduceByKey((total, orderItemSubtotal) => total + orderItemSubtotal)
scala> rdd2.take(10).foreach(println)
ORDERID,SUM(SUBTOTAL)
(18624,199.99)                                                                  
(57436,1309.8501)
(54040,1049.8301)
(7608,39.99)
(18500,449.98)
(23556,299.95)
(23776,329.98)
(58592,699.85)
(29856,1029.92)
(32676,719.91003)
//el valor minimo de subtotal.
val rdd3 = orderItems.reduceByKey((total, orderItemSubtotal) => if(total< orderItemSubtotal)total else orderItemSubtotal);
//min, max subtotal
val orderItems4 = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, (orderItem.split(",")(4).toFloat, orderItem.split(",")(4).toFloat)));
orderItems3: org.apache.spark.rdd.RDD[(Int, Float, Float)] = MapPartitionsRDD[2] at map at <console>:27

val rdd4 = orderItems4.reduceByKey((total, orderItemSubtotal) => (if(total._1< orderItemSubtotal._1)total._1 else orderItemSubtotal._1,if(total._2> orderItemSubtotal._2)total._2 else orderItemSubtotal._2));

//min,max,count
val orderItems5 = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, (orderItem.split(",")(4).toFloat, orderItem.split(",")(4).toFloat,1)));

val rdd5 = orderItems5.reduceByKey((total, orderItemSubtotal) => (if(total._1< orderItemSubtotal._1)total._1 else orderItemSubtotal._1,if(total._2> orderItemSubtotal._2)total._2 else orderItemSubtotal._2, total._3 + orderItemSubtotal._3));

val ordItAgreg = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt,( orderItem.split(",")(4).toFloat,1)))
val rdd1 = orderItems.take(10).foreach(println)
//para orderId suma de subtotal.
//lo primero es la inicializacion de los campos a calcular sumatorio de campo float orderItemSubtotal  0.0 y count 0 de los items que es un entero
//Calcule los ingresos y la cantidad de elementos para cada pedido usando aggregateByKey
//(iTotal, oisubtotal) => (iTotal._1 + oisubtotal._1, iTotal._2 + oisubtotal._2) se ejecuta en cada particion agregaciones parciales por key en la particion
//(fTotal, iTotal) => (fTotal._1 + iTotal._1, fTotal._2 + iTotal._2) se ejecuta en el driver mergeando los valores parciales
val rdd2 = orderItems.aggregateByKey((0.0, 0))((iTotal, oisubtotal) => (iTotal._1 + oisubtotal._1, iTotal._2 + oisubtotal._2),(fTotal, iTotal) => (fTotal._1 + iTotal._1, fTotal._2 + iTotal._2))

((total, orderItemSubtotal) => total + orderItemSubtotal)
scala> rdd2.take(10).foreach(println)

val ordItAgreg2 = sc.textFile("/user/cloudera/problema1").map(orderItem => (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toInt))
	

val rdd3 = ordItAgreg2.aggregateByKey((0.0))(math.min(_,_), math.min(_,_))
val rdd2 = ordItAgreg2.aggregateByKey((0.0))((iTotal, oisubtotal) 
=> ( if(iTotal._1<oisubtotal._1)iTotal._1 else oisubtotal._1) => (if(fTotal._1 < iTotal._1)fTotal._1 else iTotal._1, if(fTotal._2 > iTotal._2)fTotal._2 else iTotal._2 ));



select Host,User,Password from Users;


#problema 3 Import all tables

# CUANDO SE QUIERE IMPORTAR TODAS TABLAS NO SE ESPICIFICA NINGUNA, --WAREHOUSE-DIR: ESPECIFICA LA RUTA DE HDFS EN VEZ DE TARGET-DIR: RUTA DE HDFS , EL PARAMETRO -M 1 ES PARA QUE LAS TAREAS SE EJECUTEN EN UN SOLO HILO PARA EVITAR LAS DEPENDENCIAS
sqoop-import   --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username retail_dba  --password cloudera  --warehouse-dir /user/hive/warehouse/retail_stage.db  --compress --compression-codec snappy --as-avrodatafile  -m 1 ; 


        [root@quickstart /]# hadoop fs -ls -R /user/hive/warehouse/
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/categories
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/categories/_SUCCESS
	-rw-r--r--   1 root supergroup       1378 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/categories/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/customers
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/customers/_SUCCESS
	-rw-r--r--   1 root supergroup     469715 2018-03-15 10:28 /user/hive/warehouse/retail_stage.db/customers/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:29 /user/hive/warehouse/retail_stage.db/departments
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:29 /user/hive/warehouse/retail_stage.db/departments/_SUCCESS
	-rw-r--r--   1 root supergroup        475 2018-03-15 10:29 /user/hive/warehouse/retail_stage.db/departments/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/order_items
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/order_items/_SUCCESS
	-rw-r--r--   1 root supergroup    1526314 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/order_items/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/orders
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/orders/_SUCCESS
	-rw-r--r--   1 root supergroup     660183 2018-03-15 10:30 /user/hive/warehouse/retail_stage.db/orders/part-m-00000.avro
	drwxr-xr-x   - root supergroup          0 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db/products
	-rw-r--r--   1 root supergroup          0 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db/products/_SUCCESS
	-rw-r--r--   1 root supergroup      53592 2018-03-15 10:31 /user/hive/warehouse/retail_stage.db/products/part-m-00000.avro

#TRAEMOS LOS FICHEROS DE HDFS A LOCAL
#CUANDO SE QUIERE CARGA DE MYSQL A UNA TABLA DE HIVE SE DEBE
1.- CARGAR CON SQOOP DESDE MYSQL Y SE PUEDE GUARDAR COMO HDFS
2.- QUITAR EL FORMATO AVRO Y COPIARLO DE HDFS A LOCAL MOVER DE LOCAL(SIN FORMATO AVRO) A HFDS DE NUEVO
3.- DESDE HIVE CARGAMOS EL FICHERO ALMACENADO EN HDFS Y LO GUARDAMOS EN UNA TABLA EXTERNA CON FORMATO AVRO
[root@quickstart /]# hadoop fs -get /user/hive/warehouse/retail_stage.db/order_items/part-m-00000.avro
#CON LA HERRAMIENTA AVRO_TOOL DE CLOUDERA PASAMOS DE AVRO A OTRO FORMATO
avro-tools getschema part-m-00000.avro > orders.avsc
[root@quickstart /]# ls -l |grep avsc
-rw-r--r--   1 root root     594 Mar 15 10:27 categories.avsc
-rw-r--r--   1 root root    1509 Mar 15 10:28 customers.avsc
-rw-r--r--   1 root root     440 Mar 15 10:29 departments.avsc
-rw-r--r--   1 root root    1099 Mar 15 10:29 order_items.avsc
-rw-r--r--   1 root root    1100 Mar 15 11:52 orders.avsc
-rw-r--r--   1 root root    1041 Mar 15 10:30 products.avsc

hadoop fs -mkdir /user/hive/schemas
hadoop fs -put orders.avsc /user/hive/schemas/order =>ES IGUAL hadoop fs -copyFromLocal orders.avsc /user/hive/schemas/order

create external table orders_sqoop STORED AS AVRO LOCATION '/user/hive/warehouse/retail_stage.db/orders' TBLPROPERTIES ('avro.schema.url'='/user/hive/schemas/order/orders.avsc')
#query table in impala that shows all orders belonging to a certain day. This day is when the most orders were placed. select data from order_sqoop.
#CREAR UN QUERY SOBRE LA TABLA order_sqoop DE LOS DIAS EN LOS CUALES SE HA REALIZADP LA MAYORIA DE LOS PEDIDOS.
[root@quickstart /]# hadoop fs -cat /user/hive/schemas/order
{
  "type" : "record",
  "name" : "order_items",
  "doc" : "Sqoop import of order_items",
  "fields" : [ {
    "name" : "order_item_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_id",
    "sqlType" : "4"
  }, {
    "name" : "order_item_order_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_order_id",
    "sqlType" : "4"
  }, {
    "name" : "order_item_product_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_product_id",
    "sqlType" : "4"
  }, {
    "name" : "order_item_quantity",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_item_quantity",
    "sqlType" : "-6"
  }, {
    "name" : "order_item_subtotal",
    "type" : [ "null", "float" ],
    "default" : null,
    "columnName" : "order_item_subtotal",
    "sqlType" : "7"
  }, {
    "name" : "order_item_product_price",
    "type" : [ "null", "float" ],
    "default" : null,
    "columnName" : "order_item_product_price",
    "sqlType" : "7"
  } ],
  "tableName" : "order_items"
}
Hive> create external table orders_sqoop
    > STORED AS AVRO
    > LOCATION '/user/hive/warehouse/retail_stage.db/orders'
    > TBLPROPERTIES ('avro.schema.url'='/user/hive/schemas/order');
OK
Time taken: 2.02 seconds
#QUERY EN HIVE QUE RECOGE 5 REGISTROS DE TODOS LOS PEDIDOS QUE QUE SE REALIZARON EL DIA QUE MAS PEDIDOS SE HICIERON.
hive> select * from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5;
# HACER LO MISMO DE ANTES EN IMPALA.
impala-shell
[quickstart.cloudera:21000] > invalidate metadata ;

[quickstart.cloudera:21000] > select * from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5;
Query: select * from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5
+----------+---------------+-------------------+-----------------+
| order_id | order_date    | order_customer_id | order_status    |
+----------+---------------+-------------------+-----------------+ 
| 16078    | 1383436800000 | 65                | PENDING_PAYMENT |
| 16077    | 1383436800000 | 3407              | CLOSED          |
| 16076    | 1383436800000 | 6833              | CANCELED        |
| 16075    | 1383436800000 | 7051              | SUSPECTED_FRAUD |
| 16074    | 1383436800000 | 11930             | PENDING_PAYMENT |
+----------+---------------+-------------------+-----------------+

select oq.*,from_unixtime(oq.order_date/(365*24*60*60*1000)) from orders_sqoop as oq where oq.order_date in (select i.order_date from(select order_date,count(order_id) from orders_sqoop group by order_date order by 2 desc limit 1)as i) limit 5;

#Import orders table from mysql as text file to the destination /user/cloudera/problem5/text. 
#Fields should be terminated by a tab character (“\t”) character and lines should be terminated by new line character (“\n”).

sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problem5/text --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n';
#Import orders table from mysql  into hdfs to the destination /user/cloudera/problem5/avro. File should be stored as avro file.
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problem5/avro --as-avrodatafile ;
#Import orders table from mysql  into hdfs  to folders /user/cloudera/problem5/parquet. File should be stored as parquet file SQOOP_PARQUET.

sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table orders  --target-dir /user/cloudera/problem5/parquet --as-parquetfile ;
[root@quickstart /]# hadoop fs -ls -R /user/cloudera/problem5
drwxr-xr-x   - root cloudera          0 2018-03-19 09:47 /user/cloudera/problem5/avro
-rw-r--r--   1 root cloudera          0 2018-03-19 09:47 /user/cloudera/problem5/avro/_SUCCESS
-rw-r--r--   1 root cloudera     439146 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00000.avro
-rw-r--r--   1 root cloudera     447726 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00001.avro
-rw-r--r--   1 root cloudera     446959 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00002.avro
-rw-r--r--   1 root cloudera     447606 2018-03-19 09:47 /user/cloudera/problem5/avro/part-m-00003.avro
drwxr-xr-x   - root cloudera          0 2018-03-19 10:00 /user/cloudera/problem5/parquet
drwxr-xr-x   - root cloudera          0 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata
-rw-r--r--   1 root cloudera        184 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/descriptor.properties
-rw-r--r--   1 root cloudera        707 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/schema.avsc
drwxr-xr-x   - root cloudera          0 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/schemas
-rw-r--r--   1 root cloudera        707 2018-03-19 09:59 /user/cloudera/problem5/parquet/.metadata/schemas/1.avsc
drwxr-xr-x   - root cloudera          0 2018-03-19 10:00 /user/cloudera/problem5/parquet/.signals
-rw-r--r--   1 root cloudera          0 2018-03-19 10:00 /user/cloudera/problem5/parquet/.signals/unbounded
-rw-r--r--   1 root cloudera     147264 2018-03-19 10:00 /user/cloudera/problem5/parquet/18e8ba61-3ac0-4ab7-9784-d01e42b11d30.parquet
-rw-r--r--   1 root cloudera     147312 2018-03-19 10:00 /user/cloudera/problem5/parquet/2c528d70-cf06-42eb-9e1e-419b79ba6ee7.parquet
-rw-r--r--   1 root cloudera     147465 2018-03-19 10:00 /user/cloudera/problem5/parquet/5479e2f2-42b4-4ad8-b8c8-53f8a8c554f3.parquet
-rw-r--r--   1 root cloudera     151743 2018-03-19 10:00 /user/cloudera/problem5/parquet/6590cdab-4245-402a-94d7-c778527c9aab.parquet
drwxr-xr-x   - root cloudera          0 2018-03-19 09:44 /user/cloudera/problem5/text
-rw-r--r--   1 root cloudera          0 2018-03-19 09:44 /user/cloudera/problem5/text/_SUCCESS
-rw-r--r--   1 root cloudera     741614 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00000
-rw-r--r--   1 root cloudera     753022 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00001
-rw-r--r--   1 root cloudera     752368 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00002
-rw-r--r--   1 root cloudera     752940 2018-03-19 09:44 /user/cloudera/problem5/text/part-m-00003

#Transform/Convert data-files at /user/cloudera/problem5/avro and store the converted file at the following locations and file formats
# En un ide
  val conf = new SparkConf().setAppName("miApp").setMaster("local[2]")
  val sc=new SparkContext(conf) 
  val sqlContext = new org.apache.spark.sql.SQLContext(sc) 
#  1.-save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem5/parquet-snappy-compress
  val orders = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/format-avro")
orders.write.option("spark.io.compression.codec", "snappy").parquet("/user/cloudera/problem5/parquet-snappy-compress")
#  2.-save the data to hdfs using gzip compression as text file at /user/cloudera/problem5/text-gzip-compress
  val orderTextGzip=orders.map(x => x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)+"\t")
  orderTextGzip.saveAsTextFile("/user/cloudera/problem5/text-gzip-compress", classOf[org.apache.hadoop.io.compress.GzipCodec])
#  3.-save the data to hdfs using no compression as sequence file at /user/cloudera/problem5/sequence
  orderTextGzip.saveAsTextFile("/user/cloudera/problem5/text")
#  4.-save the data to hdfs using snappy compression as text file at /user/cloudera/problem5/text-snappy-compress
orderTextGzip.saveAsTextFile("/user/cloudera/problem5/text-snappy-compress1", classOf[org.apache.hadoop.io.compress.SnappyCodec])



#Transform/Convert data-files at /user/cloudera/problem5/parquet-snappy-compress and store the converted file at the following locations and file formats
# 1.- save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress

   val ordersParquetSinCompress = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress1")
   #DESACTIVAR COMPRESION
   sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
   #ESCRIBIR 
   val  ordersParquetSinCompressDF=ordersParquetSinCompress.write.parquet("/user/cloudera/problem5/parquet-no-compress")
   #ACTIVAR COMPRESION
   sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
# 2.- save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy
ordersParquetSinCompress.write.format("com.databricks.spark.avro").save("/user/cloudera/problem5/avro-snappy")

# Transform/Convert data-files at /user/cloudera/problem5/avro-snappy and store the converted file at the following locations and file formats
# 1.- save the data to hdfs using no compression as json file at /user/cloudera/problem5/json-no-compress
   val rddAvro=sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/avro-snappy")
   sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
   rddAvro.write.json("/user/cloudera/problem5/json-no-compress")
   scala> rddAvro.write.json("/user/cloudera/problem5/json-no-compress") 
	[Stage 0:>                                                          
	([Stage 0:>                                                         
	([Stage 0:==============>                                            
	([Stage 0:=============================>                             
	([Stage 0:===========================================================(
   sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
# 2.- save the data to hdfs using gzip compression as json file at /user/cloudera/problem5/json-gzip
   val rddAvro=sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/avro-snappy")
   rddAvro.write.option("spark.io.compression.codec", "gzip").json("/user/cloudera/problem5/json-gzip")
   scala> rddAvro.write.option("spark.io.compression.codec", "gzip").json("/user/cloudera/problem5/json-gzip")
    [Stage 1:=============================>
    
#Transform/Convert data-files at  /user/cloudera/problem5/json-gzip and store the converted file at the following locations and file formats
# 1.-save the data to as comma separated text using gzip compression at   /user/cloudera/problem5/csv-gzip
scala> var jsonData = sqlContext.read.json("/user/cloudera/problem5/json-gzip");
jsonData: org.apache.spark.sql.DataFrame = [order_customer_id: bigint, order_date: bigint, order_id: bigint, order_status: string]
scala> jsonData.map(x=>x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/cloudera/problem5/csv-gzip", classOf[org.apache.hadoop.io.compress.GzipCodec])

#Using spark access data at /user/cloudera/problem5/sequence and stored it back to hdfs using no compression as ORC file to HDFS to destination /user/cloudera/problem5/orc
 sqoop import --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db"   --password cloudera   --username retail_dba   --table orders   --as-avrodatafile   --target-dir /user/cloudera/problem5/avro1   -m 1
 import com.databricks.spark.avro
 var dataFile = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem5/avro1");
 dataFile.map(x=> (x(0).toString,x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3))).saveAsSequenceFile("/user/cloudera/problem5/sequence");
 scala> var seqData = sc.sequenceFile("/user/cloudera/problem5/sequence/", classOf[org.apache.hadoop.io.Text], classOf[org.apache.hadoop.io.Text]);
 seqData: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.Text, org.apache.hadoop.io.Text)] = /user/cloudera/problem5/sequence/ HadoopRDD[53] at sequenceFile at <console>:28
 
 seqData.map(x => { var d = x._2.toString.split("\t"); (d(0),d(1),d(2),d(3)) }).toDF().write.orc("/user/cloudera/problem5/orc");


#SQOOP JOB

sqoop job --create incjob -- import --connect jdbc:mysql://localhost:3306/test --driver com.mysql.jdbc.Driver --username it1 --password hadoop --table st1 --incremental lastmodified -check-column ts --target-dir sqin -m 1 --merge-key id


#Problem Scenario 3: Please accomplish following activities. 
#1.  Create a table region with following structure. However, underline file format should be parquet in HDFS.
#	r_regionkey smallint,
#	r_name      string,
#	r_comment   string,
#	r_nations   array<struct<n_nationkey:smallint,n_name:string,n_comment:string>> 

hive> Create external table if not exists Region
     (
     r_regionkey smallint,
     r_name string,
     r_comment string,
     r_nations array <struct<n_nationkey:smallint,n_name:string,n_comment:string>> 
     )ROW FORMAT DELIMITED
     FIELDS TERMINATED  BY '|'
     COLLECTION ITEMS TERMINATED BY ','
     STORED AS PARQUET;
#2. Once table is created , load data in this table from region.csv
#Data for region table : region.csv
r_regionkey|r_name|r_comment|r_nations &amp;lt;n_nationkey,n_name,n_comment>
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for HadoopExam.com|1,United States,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for Training4Exam.com|2,Canada,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|3,Cuba,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for Training4Exam.com|17,Costa Rica,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|24,Panama,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|8,India,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|9,China,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|12,Japan,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|18,Russia,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|21,Israel,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|6,Austria,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|7,Bulgaria,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|19,Belgium,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|22,Croatia,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|23,Denmark,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|4,Saudi Arabia,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|10,Yemen,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|11,Oman,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|13,Kuwait,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|20,Qatar,Reference site http://www.QuickTechie.com#





[root@quickstart /]#echo "r_regionkey|r_name|r_comment|r_nations &amp;lt;n_nationkey,n_name,n_comment>
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for HadoopExam.com|1,United States,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for Training4Exam.com|2,Canada,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|3,Cuba,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for Training4Exam.com|17,Costa Rica,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|24,Panama,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|8,India,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|9,China,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|12,Japan,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|18,Russia,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|21,Israel,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|6,Austria,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|7,Bulgaria,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|19,Belgium,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|22,Croatia,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|23,Denmark,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|4,Saudi Arabia,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|10,Yemen,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|11,Oman,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|13,Kuwait,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|20,Qatar,Reference site http://www.QuickTechie.com#
" > Region.csv

[root@quickstart /]# hadoop fs -mkdir /user/cloudera/hadoop
[root@quickstart /]# hadoop fs -mkdir /user/cloudera/hadoop/exam
[root@quickstart /]# awk 'NR != 1 {print}' Region.csv | hdfs dfs -put - /user/cloudera/hadoop/exam/region.csv

[root@quickstart /]# hadoop fs -cat /user/cloudera/hadoop/exam/region.csv
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
[root@quickstaawk 'NR != 1 {print}' Region.csv | hdfs dfs -put -s -cat  /user/cloudera/hadoop/exam/region.c
[root@quickstart /]#  hdfs dfs -cat /user/cloudera/hadoop/exam/region.csv
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for HadoopExam.com|1,United States,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for Training4Exam.com|2,Canada,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|3,Cuba,Reference site http://www.QuickTechie.com
2|AMERICA|Average Business Region for Training4Exam.com|17,Costa Rica,Reference site http://www.HadoopExam.com
2|AMERICA|Average Business Region for HadoopExam.com|24,Panama,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|8,India,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|9,China,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|12,Japan,Reference site http://www.QuickTechie.com
3|ASIA|Best Business Region for HadoopExam.com|18,Russia,Reference site http://www.HadoopExam.com
3|ASIA|Best Business Region for Training4Exam.com|21,Israel,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|6,Austria,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|7,Bulgaria,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|19,Belgium,Reference site http://www.HadoopExam.com
4|EUROPE|Low sale Business Region for Training4Exam.com|22,Croatia,Reference site http://www.QuickTechie.com
4|EUROPE|Low sale Business Region for HadoopExam.com|23,Denmark,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|4,Saudi Arabia,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|10,Yemen,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|11,Oman,Reference site http://www.QuickTechie.com
5|MIDDLE EAST|Ok Ok sale Business Region for Training4Exam.com|13,Kuwait,Reference site http://www.HadoopExam.com
5|MIDDLE EAST|Ok Ok sale Business Region for HadoopExam.com|20,Qatar,Reference site http://www.QuickTechie.com#



scala> val textFile = sc.textFile("hdfs:///user/cloudera/hadoop/exam/region.csv");

textFile: org.apache.spark.rdd.RDD[String] = hdfs:///user/cloudera/hadoop/exam/region.csv MapPartitionsRDD[1] at textFile at <console>:27

scala> textFile.take(5).foreach(println)
1|AFRICA|Good Business Region for HadoopExam.com|0,Cameroon,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|5,Egypt,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|14,Namibia,Reference site http://www.QuickTechie.com
1|AFRICA|Good Business Region for Training4Exam.com|15,Zimbabwe,Reference site http://www.HadoopExam.com
1|AFRICA|Good Business Region for HadoopExam.com|16,Uganda,Reference site http://www.QuickTechie.com
scala> import sqlContext.implicits._
scala> val df = textFile.toDF("line")
scala> df.write.parquet("hdfs:///user/cloudera/hadoop/exam/parquetRegion");


hive> load data inpath '/user/cloudera/hadoop/exam/parquetRegion' into table region;

hive> create table tempresgion(data string);
hive> load data inpath '/user/cloudera/hadoop/exam/region.csv' into table tempresgion;
hive> select split(data,'\\|')[0] r_regionkey,
    > split(data,'\\|')[1] r_name,
    > split(data,'\\|')[2] r_commet,
    > split(split(data,'\\|')[3],",")[0] n_nationKey,
    > split(split(data,'\\|')[3],",")[1] n_name,
    > split(split(data,'\\|')[3],",")[2] n_comment 
    > from 
    > tempresgion limit 5;
OK
1	AFRICA	Good Business Region for HadoopExam.com	0	Cameroon	Reference site http://www.QuickTechie.com
1	AFRICA	Good Business Region for Training4Exam.com	5	Egypt	Reference site http://www.HadoopExam.com
1	AFRICA	Good Business Region for HadoopExam.com	14	Namibia	Reference site http://www.QuickTechie.com
1	AFRICA	Good Business Region for Training4Exam.com	15	Zimbabwe	Reference site http://www.HadoopExam.com
1	AFRICA	Good Business Region for HadoopExam.com	16	Uganda	Reference site http://www.QuickTechie.com
Time taken: 0.127 seconds, Fetched: 5 row(s)

hive> insert overwrite table region 
    > select split(data,'\\|')[0] r_regionkey,
    > split(data,'\\|')[1] r_name,
    > split(data,'\\|')[2] r_commet,
    > array(named_struct("n_nationkey",cast(split(split(data,'\\|')[3],",")[0] as smallint),"n_name",split(split(data,'\\|')[3],",")[1],"n_comment",split(split(data,'\\|')[3],",")[2] )) from tempresgion;
Query ID = root_20180405104747_79e1b18a-be6b-48f1-b2a3-cb15c2658c99
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1522922964151_0006, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1522922964151_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1522922964151_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2018-04-05 14:12:59,110 Stage-1 map = 0%,  reduce = 0%
2018-04-05 14:13:05,401 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.28 sec
MapReduce Total cumulative CPU time: 2 seconds 280 msec
Ended Job = job_1522922964151_0006
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/region/.hive-staging_hive_2018-04-05_14-12-51_476_533907710585064828-1/-ext-10000
Loading data to table default.region
Table default.region stats: [numFiles=1, numRows=26, totalSize=2432, rawDataSize=104]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.28 sec   HDFS Read: 7580 HDFS Write: 2503 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 280 msec
OK
Time taken: 15.416 seconds
Time taken: 0.131 seconds, Fetched: 26 row(s)
hive> select * from region limit 10;
OK
1	AFRICA	Good Business Region for HadoopExam.com	[{"n_nationkey":0,"n_name":"Cameroon","n_comment":"Reference site http://www.QuickTechie.com"}]
1	AFRICA	Good Business Region for Training4Exam.com	[{"n_nationkey":5,"n_name":"Egypt","n_comment":"Reference site http://www.HadoopExam.com"}]
1	AFRICA	Good Business Region for HadoopExam.com	[{"n_nationkey":14,"n_name":"Namibia","n_comment":"Reference site http://www.QuickTechie.com"}]
1	AFRICA	Good Business Region for Training4Exam.com	[{"n_nationkey":15,"n_name":"Zimbabwe","n_comment":"Reference site http://www.HadoopExam.com"}]
1	AFRICA	Good Business Region for HadoopExam.com	[{"n_nationkey":16,"n_name":"Uganda","n_comment":"Reference site http://www.QuickTechie.com"}]
2	AMERICA	Average Business Region for HadoopExam.com	[{"n_nationkey":1,"n_name":"United States","n_comment":"Reference site http://www.HadoopExam.com"}]
2	AMERICA	Average Business Region for Training4Exam.com	[{"n_nationkey":2,"n_name":"Canada","n_comment":"Reference site http://www.HadoopExam.com"}]
2	AMERICA	Average Business Region for HadoopExam.com	[{"n_nationkey":3,"n_name":"Cuba","n_comment":"Reference site http://www.QuickTechie.com"}]
2	AMERICA	Average Business Region for Training4Exam.com	[{"n_nationkey":17,"n_name":"Costa Rica","n_comment":"Reference site http://www.HadoopExam.com"}]
2	AMERICA	Average Business Region for HadoopExam.com	[{"n_nationkey":24,"n_name":"Panama","n_comment":"Reference site http://www.HadoopExam.com"}]
Time taken: 0.115 seconds, Fetched: 10 row(s)
 

#Crear un tabla que contenga valores numerícos que la columna se llame value y otra columna que se llame property 
y tenga el valor es par(even) ó impar(odd).  

En 192.168.2.137 => sudo apt-get intall openssh-server; nmap localhost => abrir puerto 22
En 172.17.0.2 => sudo apt-get intall openssh-server; service sshd status; service sshd start.

scp -v cristina.cano@192.168.2.137:/home/cristina.cano/hadoopexamintFile.csv root@172.17.0.2:/home/cloudera

hdfs dfs -mkdir /user/cloudera/hadoopexam

hdfs dfs -put /home/cloudera/hadoopexamintFile.csv /user/cloudera/hadoopexam
hive> create external table hadoopexamint1 (value int, property string) row format delimited fields terminated by ',' stored as textfile  ;
hive> load data inpath '/user/cloudera/hadoopexam/hadoopexamintFile.csv' into table hadoopexamint;

impala-shell
[quickstart.cloudera:21000] > show databases;
[quickstart.cloudera:21000] > use default;
#actualizar la metainforamcion de las tablas creadas en HIVE.
[quickstart.cloudera:21000] > show tables;Query: show tables
+----------------+
| name           |
+----------------+
| hadoopexamint  |
| hadoopexamint1 |
+----------------+
Fetched 2 row(s) in 0.01s
[quickstart.cloudera:21000] > select property,group_concat(ltrim(cast(value as string)),"|") from hadoopexamint1 group by property;
Query: select property,group_concat(ltrim(cast(value as string)),"|") from hadoopexamint1 group by property
+----------+---------------------------------------------------------+
| property | group_concat(ltrim(cast(value as string)), '|')         |
+----------+---------------------------------------------------------+
| odd      | 1|3|5|7|9|11|13|15|17|19|21|23|25|27|29|31|33|35|37|39  |
| even     | 2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40 |
+----------+---------------------------------------------------------+
Fetched 2 row(s) in 0.32s




IMPALA SPARK JDBC 

13
down vote
accepted
val JDBCDriver = "com.cloudera.impala.jdbc41.Driver"
val ConnectionURL = "jdbc:impala://url.server.net:21050/default;auth=noSasl"

Class.forName(JDBCDriver).newInstance
val con = DriverManager.getConnection(ConnectionURL)
val stmt = con.createStatement()
val rs = stmt.executeQuery(query)

val resultSetList = Iterator.continually((rs.next(), rs)).takeWhile(_._1).map(r => {
    getRowFromResultSet(r._2) // (ResultSet) => (spark.sql.Row)
}).toList

sc.parallelize(resultSetList)

[quickstart.cloudera:21000] > invalidate metadata;

If you are using JDBC-enabled applications on hosts outside the CDH cluster
, you cannot use the CDH install procedure on the non-CDH hosts. I
nstall the JDBC driver on at least one CDH host using the preceding procedure.
Then download the JAR files to each client machine that will use JDBC with Impala:

commons-logging-X.X.X.jar
  hadoop-common.jar
  hive-common-X.XX.X-cdhX.X.X.jar
  hive-jdbc-X.XX.X-cdhX.X.X.jar
  hive-metastore-X.XX.X-cdhX.X.X.jar
  hive-service-X.XX.X-cdhX.X.X.jar
  httpclient-X.X.X.jar
  httpcore-X.X.X.jar
  libfb303-X.X.X.jar
  libthrift-X.X.X.jar
  log4j-X.X.XX.jar
  slf4j-api-X.X.X.jar
  slf4j-logXjXX-X.X.X.jar

#instalar spark2.2 en maquina vistual:

PARK 2.2 Installation Setup on Cloudera VM

Step 1: Download a quickstart_vm from the link:
Prefer a vmware platform as it is easy to use, anyways all the options are viable.
Size is around 5.4gb of the entire tar file. We need to provide the business email id as it won’t accept personal email ids. 


Step 2: The virtual environment requires around 8gb of RAM, please allocate sufficient memory to avoid performance glitches.


Step 3: Please open the terminal and switch to root user as:
         su root
         password: cloudera

Step 4: Cloudera provides java –version 1.7.0_67 which is old and does not match with our needs. To avoid java related exceptions, please install java with the following commands:
(a). Downloading Java:
wget -c --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz

(b). Switch to /usr/java/ directory with “cd /usr/java/” command.

(c). cp the java download tar file to the /usr/java/ directory.

(d). Untar the directory with “tar –zxvf jdk-8u31-linux-x64.tar.gz”

(e). Open the profile file with the command “vi ~/.bash_profile” 

(f). export JAVA_HOME to the new java directory.
       “export JAVA_HOME=/usr/java/jdk1.8.0_131”

       Save and Exit.


(g). In order to reflect the above change, following command needs to be executed on the shell:
       source ~/.bash_profile

Step 5:  The Cloudera VM provides spark 1.6 version by default. However, 1.6 API’s are old and do not match with production environments. In that case, we need to download and manually install Spark 2.2.

(a). Switch to /opt/  directory with the command:
“cd /opt/”

(b). Download spark with the command:
wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz

(c). Untar the spark tar with the following command:
tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz

(d). We need to define some environment variables as default settings:
Please open a file with the following command:
vi /opt/spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh
Paste the following configurations in the file:
SPARK_MASTER_IP=192.168.50.1
SPARK_EXECUTOR_MEMORY=512m
SPARK_DRIVER_MEMORY=512m
SPARK_WORKER_MEMORY=512m
SPARK_DAEMON_MEMORY=512m
Save and exit


IMPORT TO DEVELOP:
com.cloudera.impala.jdbc41.Driver
com.cloudera.impala.jdbc41.DataSource
com.cloudera.impala.jdbc4.Driver
com.cloudera.impala.jdbc4.DataSource
com.cloudera.impala.jdbc3.Driver
com.cloudera.impala.jdbc3.DataSource

jdbc:hive2://myhost.example.com:21050/;auth=noSasl
df.write.mode("append").parquet("/user/hive/warehouse/Mytable")












